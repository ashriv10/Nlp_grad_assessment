{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.csee.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu\"\n",
        "!pip install conllu\n",
        "import conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlEyEWanLXmJ",
        "outputId": "bf2087d8-3a5d-44a7-fb50-6f9f64d8c365"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-11 07:38:48--  https://www.csee.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "Resolving www.csee.umbc.edu (www.csee.umbc.edu)... 23.185.0.4, 2620:12a:8000::4, 2620:12a:8001::4\n",
            "Connecting to www.csee.umbc.edu (www.csee.umbc.edu)|23.185.0.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://redirect.cs.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu [following]\n",
            "--2022-12-11 07:38:48--  https://redirect.cs.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "Resolving redirect.cs.umbc.edu (redirect.cs.umbc.edu)... 130.85.36.80\n",
            "Connecting to redirect.cs.umbc.edu (redirect.cs.umbc.edu)|130.85.36.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13301116 (13M) [text/plain]\n",
            "Saving to: ‘en_ewt-ud-train.conllu.8’\n",
            "\n",
            "en_ewt-ud-train.con 100%[===================>]  12.68M  77.0MB/s    in 0.2s    \n",
            "\n",
            "2022-12-11 07:38:49 (77.0 MB/s) - ‘en_ewt-ud-train.conllu.8’ saved [13301116/13301116]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.8/dist-packages (4.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "7LAK2Vu4SIIR"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "# https://github.com/EmilStenstrom/conllu/\n",
        "import conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLglm0_fTWMr",
        "outputId": "ef61a067-c0fe-41b3-c389-9b3d788eaa28"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.8/dist-packages (4.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.csee.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu\"\n",
        "with open('en_ewt-ud-train.conllu') as data:\n",
        "  traindata = [i for i in conllu.parse_incr(data)]\n",
        "print(len(traindata))\n",
        "traindata[0][0]\n",
        "\n",
        "            \n",
        "   \n",
        "#returns : \n",
        "#TokenList<Al, -, Zaman, :, American, forces, killed, Shaikh, Abdullah, al, -, Ani, ,, the, preacher, at, the, mosque, in, the, town, of, Qaim, ,, near, the, Syrian, border, ., metadata={newdoc id: \"weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000\", sent_id: \"weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000-0001\", text: \"Al-Zaman : American forces killed Shaikh Abdullah al-Ani, the preacher at the mosque in the town of Qaim, near the Syrian border.\"}>\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6f-WEIrCknG",
        "outputId": "bafe8eee-bf0a-4ed2-80bf-241e7b1836cd"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-11 07:38:56--  https://www.csee.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "Resolving www.csee.umbc.edu (www.csee.umbc.edu)... 23.185.0.4, 2620:12a:8000::4, 2620:12a:8001::4\n",
            "Connecting to www.csee.umbc.edu (www.csee.umbc.edu)|23.185.0.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://redirect.cs.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu [following]\n",
            "--2022-12-11 07:38:56--  https://redirect.cs.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "Resolving redirect.cs.umbc.edu (redirect.cs.umbc.edu)... 130.85.36.80\n",
            "Connecting to redirect.cs.umbc.edu (redirect.cs.umbc.edu)|130.85.36.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13301116 (13M) [text/plain]\n",
            "Saving to: ‘en_ewt-ud-train.conllu.9’\n",
            "\n",
            "\ren_ewt-ud-train.con   0%[                    ]       0  --.-KB/s               \ren_ewt-ud-train.con  95%[==================> ]  12.07M  60.3MB/s               \ren_ewt-ud-train.con 100%[===================>]  12.68M  61.7MB/s    in 0.2s    \n",
            "\n",
            "2022-12-11 07:38:57 (61.7 MB/s) - ‘en_ewt-ud-train.conllu.9’ saved [13301116/13301116]\n",
            "\n",
            "12543\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 1,\n",
              " 'form': 'Al',\n",
              " 'lemma': 'Al',\n",
              " 'upos': 'PROPN',\n",
              " 'xpos': 'NNP',\n",
              " 'feats': {'Number': 'Sing'},\n",
              " 'head': 0,\n",
              " 'deprel': 'root',\n",
              " 'deps': [('root', 0)],\n",
              " 'misc': {'SpaceAfter': 'No'}}"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_map={}\n",
        "def form_label(traindata):# creating the map of unique labels with their upos and arc labels\n",
        "    dummylist = []   \n",
        "    counter=0\n",
        "    for inst in traindata:             ####################concatenating words into dictionary\n",
        "      for k in inst:\n",
        "         if k['form'] not in dummylist:\n",
        "            dummylist.append(k['form'])\n",
        "         if k['upos'] not in dummylist:\n",
        "            dummylist.append(k['upos'])\n",
        "         if k['deprel'] not in dummylist:\n",
        "            dummylist.append(k['deprel'])               \n",
        "    for i in range(len(dummylist)):\n",
        "       word_map[dummylist[i]] = counter\n",
        "       counter+=1\n",
        "\n",
        "form_label(traindata)"
      ],
      "metadata": {
        "id": "pts7IQCFIOWt"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "class sentencestack(object):  # define class for making object for each sentence\n",
        "    def __init__(self, zipped, arc_label,pos_label):\n",
        "        self.stack = []\n",
        "        self.buffer = []\n",
        "        self.dependencylist = []\n",
        "        self.zipped = zipped\n",
        "        self.arc_label=arc_label\n",
        "        self.pos_label = pos_label\n"
      ],
      "metadata": {
        "id": "9elsRLP4ye7_"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create stack object for each sentence in the training dataset  \n",
        "#buffer=[]\n",
        "#dependency_relation =[]\n",
        "#final_stacklist=[]\n",
        "#final_transitlist=[]\n",
        "\n",
        "\n",
        "#building parse step\n",
        "\n",
        "def Parse(data):\n",
        "  list_of_objects =[]\n",
        "  for i in data:   #define sentence [0] == root and deps=-1 intially for every sentence\n",
        "    arc_label ={}\n",
        "    pos_label ={}\n",
        "    sentence =[\"root\"]\n",
        "    deps =[-1]\n",
        "    for word in i:\n",
        "       sentence.append(word['form'])\n",
        "       deps.append(word['head'])\n",
        "       arc_label[word['form']]=word['deprel'] # setting dictionary within each object which depicts the word with corresponding upos and arc label\n",
        "       pos_label[word['form']] = word['upos']  \n",
        "    zipped=zip(sentence,deps)  \n",
        "    objectstack= sentencestack(zipped, arc_label,pos_label) \n",
        "    list_of_objects.append(objectstack)  \n",
        "  return list_of_objects   \n",
        "\n",
        "def start_parse(list_of_objects):  # returns the list of encodings of the sentences and their corresponding labels for training for a single batch\n",
        "  tr_batch=[]\n",
        "  tr_label=[]\n",
        "  for obj in list_of_objects:\n",
        "    list1,labels = actual_parse(obj) \n",
        "    for i in range(len(list1)): #######################list1 is a list of stacks and labels consists of corresponding stack labels\n",
        "      ftlabel = []\n",
        "      ftdeprel=[]\n",
        "      for p in list1[i]:        ############################form a list of words and their corresponding arc and upos label in a ftvector list \n",
        "        if p != \"root\":\n",
        "          ftlabel.append(obj.arc_label[p]) \n",
        "          ftdeprel.append(obj.pos_label[p])\n",
        "      \n",
        "      st_labels = ' '.join(ftlabel)    \n",
        "      st_words = ' '.join(list1[i])\n",
        "      st_pos = ' '.join(ftdeprel)\n",
        "      finalstr = st_words +\" \"+ st_labels +\" \"+st_pos\n",
        "                                     #ftvector = form_embeddings(ftvector)  ######################### form a feature vector of a single stack from stacklist\n",
        "      tr_batch.append(finalstr)\n",
        "      tr_label.append(labels[i])\n",
        "  \n",
        "  # for t in range(len(tr_label)):      #######################################encode traning labels \n",
        "  #   tr_label[t] = final_train_labels[tr_label[t]]\n",
        "\n",
        "  return tr_batch, tr_label ####################return encoding of the all the parse sentences states with their corresponding labels for training    \n",
        " \n",
        "  \n",
        "def actual_parse(s):   # arc-eager parser, parsing every sentence\n",
        "      obj =s\n",
        "      index=0\n",
        "      final_list = list(obj.zipped)\n",
        "      iter=0\n",
        "      item_dict={}\n",
        "      item_list=[]\n",
        "      transitval =\"\"\n",
        "      dummylist=[]\n",
        "      transit=[]\n",
        "      for i in final_list:      # add words to buffer\n",
        "        obj.buffer.append(i[0])\n",
        "        item_list.append(i[0])\n",
        "\n",
        "      for i in final_list:\n",
        "        item_dict[i[0]] = i[1]   # make dictionary with the values of head corresponding to each word\n",
        "      \n",
        "      lenbuffer = len(obj.buffer)\n",
        "\n",
        "      while(len(obj.buffer)>0 ):\n",
        "        iter+=1               \n",
        "        if (len(obj.buffer)==0):\n",
        "          break\n",
        "        if(iter > 20):  # break loop to prevent infinite loop in certain cases\n",
        "          break  \n",
        "        \n",
        "        if obj.buffer[0] == \"root\" : # check if buffer[iter] == root,append it to the stack and pop from buffer\n",
        "          obj.stack.append(\"root\")\n",
        "          obj.buffer.pop(0)\n",
        "          dummylist.append(list(obj.stack))\n",
        "          transit.append(\"S\")\n",
        "         \n",
        "        else:\n",
        "          word1 = obj.stack[-1]\n",
        "          index1= item_dict[word1]\n",
        "          if index1 != None:\n",
        "            head1 = item_list[index1]\n",
        "          else:\n",
        "            head1= None  \n",
        "          word = obj.buffer[0]\n",
        "          index = item_dict[word]\n",
        "          if index != None:\n",
        "            head = item_list[index]\n",
        "          else:\n",
        "            head = None  \n",
        "          \n",
        "          if (obj.buffer[0] == head1 and obj.stack[-1]!=\"root\"):\n",
        "            transitval = \"LA\"\n",
        "            \n",
        "          if (obj.stack[-1] == head):\n",
        "            transitval =\"RA\"\n",
        "\n",
        "          if(obj.stack[-1] != head and obj.buffer[0] !=head1):\n",
        "            transitval = \"S\"\n",
        "\n",
        "        if (transitval ==\"RA\"):\n",
        "            obj.stack.append(obj.buffer[0]) \n",
        "            obj.buffer.pop(0)\n",
        "            dummylist.append(list(obj.stack))\n",
        "            transit.append(\"RA\")\n",
        "        \n",
        "        elif (transitval == \"LA\"):  \n",
        "            obj.stack.pop(-1)\n",
        "            dummylist.append(list(obj.stack))\n",
        "            transit.append(\"LA\")\n",
        "        \n",
        "        elif (transitval == \"S\"):\n",
        "            obj.stack.append(obj.buffer[0]) \n",
        "            obj.buffer.pop(0)\n",
        "            dummylist.append(list(obj.stack))\n",
        "            transit.append(\"S\")  \n",
        "        if(len(obj.stack)<1):    # to prevent empty stack \n",
        "          break \n",
        "      return dummylist,transit       \n",
        "     \n",
        "#Parse(traindata)\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "kTUAttVTJU-9"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model over traindataset\n",
        "import math\n",
        "import numpy\n",
        "n_features = 0\n",
        "def return_train_data(traindata):\n",
        "      sentence_object = Parse(traindata)     #########################Parse each sentence for this batch ####################################\n",
        "      train_data, train_labels = start_parse(sentence_object) ####################### a list of feature embeddings of stack states amd their correspondig label \n",
        "      print(len(train_data))\n",
        "      print(len(train_labels))\n",
        "      print(\"training started\")\n",
        "      #n_features = train_data.shape\n",
        "      return train_data, train_labels\n",
        "      "
      ],
      "metadata": {
        "id": "Rb10h8nO_UEM"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### define model #################################\n",
        "import tensorflow as tf\n",
        "import torch \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "final_train_labels = {\"LA\":0,\"RA\":1,\"S\":2} \n",
        "X_train, Y_train = return_train_data(traindata)      ###################################return traindata ############################\n",
        "tokenizer = Tokenizer(num_words=len(word_map))\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "seq = tokenizer.texts_to_sequences(X_train)\n",
        "print(seq[:2])\n",
        "x_train_new = tf.keras.utils.pad_sequences(seq, maxlen=50)\n",
        "x_train_new.shape\n",
        "Y1_train=[]\n",
        "\n",
        "for i in range(len(Y_train)):                        #############################################################convert labels to numbers\n",
        "  Y1_train.append(final_train_labels[Y_train[i]] ) \n",
        "\n",
        "                                                    #####build a sequential model #######################################################\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(19000,500)  ,\n",
        "  tf.keras.layers.LSTM(500, dropout=0.2),\n",
        "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
        "  #tf.keras.layers.Dropout(0.8),\n",
        "  tf.keras.layers.Dense(3, activation =\"softmax\")\n",
        "])\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt,\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train_new,np.array(Y1_train), epochs=10, batch_size=50, verbose=1)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7frTfAt1f4h",
        "outputId": "3659700d-8981-4f18-af1f-85dab049501f"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194525\n",
            "194525\n",
            "training started\n",
            "[[1], [1, 248, 1, 10]]\n",
            "Epoch 1/10\n",
            "3891/3891 [==============================] - 60s 15ms/step - loss: 1.2325 - accuracy: 0.3948\n",
            "Epoch 2/10\n",
            "3891/3891 [==============================] - 58s 15ms/step - loss: 0.9559 - accuracy: 0.5777\n",
            "Epoch 3/10\n",
            "3891/3891 [==============================] - 58s 15ms/step - loss: 0.9523 - accuracy: 0.5777\n",
            "Epoch 4/10\n",
            "3891/3891 [==============================] - 58s 15ms/step - loss: 0.9520 - accuracy: 0.5777\n",
            "Epoch 5/10\n",
            "3891/3891 [==============================] - 58s 15ms/step - loss: 0.9519 - accuracy: 0.5777\n",
            "Epoch 6/10\n",
            "3891/3891 [==============================] - 58s 15ms/step - loss: 0.9518 - accuracy: 0.5777\n",
            "Epoch 7/10\n",
            "3891/3891 [==============================] - 58s 15ms/step - loss: 0.9517 - accuracy: 0.5777\n",
            "Epoch 8/10\n",
            "3891/3891 [==============================] - 58s 15ms/step - loss: 0.9516 - accuracy: 0.5777\n",
            "Epoch 9/10\n",
            "3891/3891 [==============================] - 58s 15ms/step - loss: 0.9515 - accuracy: 0.5777\n",
            "Epoch 10/10\n",
            "3891/3891 [==============================] - 58s 15ms/step - loss: 0.9514 - accuracy: 0.5777\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd4dafbff40>"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYSudjpoILzo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}