{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7TGf1IyjkER"
      },
      "outputs": [],
      "source": [
        "!wget \"https://www.csee.umbc.edu/courses/undergraduate/473/f22/materials/a1/data/UD_English-EWT/en_ewt-ud-train.conllu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5wDPO1Xj2f3",
        "outputId": "5d06bc18-db4f-4b8a-8229-0e8a080efbfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.7/dist-packages (4.5.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install conllu\n",
        "import conllu as con\n",
        "from itertools import chain\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzyiNurgj5zU"
      },
      "outputs": [],
      "source": [
        "with open('en_ewt-ud-train.conllu') as train_file:\n",
        "  train_data = [train for train in con.parse_incr(train_file)]\n",
        "\n",
        "upos_data=[]\n",
        "deprel_data=[]\n",
        "form_data=[]\n",
        "\n",
        "for word in train_data:\n",
        "  for val in word:\n",
        "    if (val['form']) not in form_data:\n",
        "      form_data.append(val['form'])\n",
        "    if (val['upos']) not in upos_data:\n",
        "      upos_data.append(val['upos']) \n",
        "    if (val['deprel']) not in deprel_data:\n",
        "      deprel_data.append(val['deprel'])  \n",
        "\n",
        "upos_dict={}\n",
        "form_dict={}\n",
        "deprel_dict={}\n",
        "counter=1\n",
        "for f in form_data:\n",
        "  form_dict[f] = counter\n",
        "  counter+=1\n",
        "\n",
        "counter=1\n",
        "for u in upos_data:\n",
        "  upos_dict[u]=counter\n",
        "  counter+=1\n",
        "counter=1  \n",
        "for d in deprel_data:\n",
        "  deprel_dict[d]=counter\n",
        "  counter+=1  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# creating word tensors embeddings which will be refered by the model \n",
        "\n",
        "embeds_upos = nn.Embedding(len(upos_dict)+1,10)  \n",
        "embeds_form = nn.Embedding(len(form_dict)+1,50)\n",
        "embeds_deprel = nn.Embedding(len(deprel_data)+1,10)\n",
        "\n",
        "upos_tensor_embed=[]\n",
        "form_tensor_embed=[]\n",
        "deprel_tensor_embed=[]\n",
        "\n",
        "for key,val in upos_dict.items():\n",
        "  lookup_tensor = torch.tensor((val),dtype=torch.long)  \n",
        "  upos_tensor_embed.append(embeds_upos(lookup_tensor))\n",
        "for key,val in form_dict.items():\n",
        "  lookup_tensor=torch.tensor((val),dtype=torch.long)  \n",
        "  form_tensor_embed.append(embeds_form(lookup_tensor))\n",
        "for key,val in deprel_dict.items():\n",
        "  lookup_tensor=torch.tensor((val),dtype=torch.long) \n",
        "  deprel_tensor_embed.append(embeds_deprel(lookup_tensor))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpIWMpT61_ki",
        "outputId": "ad7ab900-9484-4682-ae35-2dc24cb4ca3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.7656, -0.5228, -0.6567, -0.1493,  0.4164,  0.5512,  0.1863, -2.1181,\n",
            "        -1.3436, -1.1894], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# this class performs transition after applying model.predict which predicts the next transition based on the current state of buffer, stack and return a list of dependencies \n",
        "\n",
        "class Sentence_Parse(object):\n",
        "    def __init__(self, sentence):\n",
        "\n",
        "        self.stack = [\"ROOT\"]\n",
        "        self.buffer = sentence[:]\n",
        "        self.dependencies = []\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "\n",
        "    def parse_step(self, transition_action):\n",
        "      # Performs a transition for the current state , which could be either \"Shift\", \"Left Arc\" or \"Right Arc\"\n",
        "        \n",
        "        if self.buffer and transition_action == \"shift\":\n",
        "            self.stack.append(self.buffer.pop(0))\n",
        "        elif len(self.stack) >= 2 and transition_action == \"LeftArc\":\n",
        "            self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
        "            self.stack.pop(-2)\n",
        "        elif len(self.stack) >= 2 and transition_action == \"RightArc\":\n",
        "            self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
        "            self.stack.pop(-1)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def call_parse_step(self, transitions):\n",
        "        # for every transition state apply parse_sentence\n",
        "        for tr in transitions:\n",
        "            self.parse_step(tr)\n",
        "        return self.dependencies\n",
        "\n",
        "\n",
        "def batch_parse(sentences, parse_model, batch_size):\n",
        "    #Parses a list of sentences in minibatches using a model.\n",
        "    dependencies = []\n",
        "\n",
        "    #assert batch_size != 0\n",
        "\n",
        "    stparse_objects = [Sentence_Parse(s) for s in sentences]\n",
        "    parses = stparse_objects\n",
        "\n",
        "    while parses:\n",
        "        sentence_batch = parses[:batch_size]\n",
        "        while sentence_batch:\n",
        "            transitions = parse_model.predict(sentence_batch)\n",
        "            # print(transitions)\n",
        "            for parse_obj,transition in zip(sentence_batch,transitions):\n",
        "                parse_obj.call_parse_step(transition)\n",
        "            sentence_batch = [parser for pr in sentence_batch if len(parse_obj.stack) > 1 or parse_obj.buffer]\n",
        "            # print(len(batch_parser))\n",
        "        sentence_batch = sentence_batch[batch_size:]\n",
        "    \n",
        "    dependencies = [parse_obj.dependencies for pr in stparse_objects]\n",
        "\n",
        "    return dependencies"
      ],
      "metadata": {
        "id": "g0fqkixNAbUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
  
  
import time
import pickle
import torch.nn.functional as F
import os
import torch
import torch.nn as nn


# intiliazing model with the embeddings size , output vector probability class , hidden layer size and weights

class ParserModel(nn.Module):
    
    def __init__(self, feature_embeddings, feature_vector=50,
        hiddenlayer_size=50, output_class=3, dropout_prob=0.5):
        
        super(ParserModel, self).__init__()
        self.dropout_prob = dropout_prob
        self.output_class = output_class
        self.embedding_size = feature_embeddings.shape[1]
        self.feature_vector = feature_vector
        self.hiddenlayer_size = hiddenlayer_size
        self.embeddings_weights = nn.Parameter(torch.tensor(feature_embeddings))
}
